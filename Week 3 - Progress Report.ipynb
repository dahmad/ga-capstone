{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Report\n",
    "#### 1. Data in hand?\n",
    "\n",
    "I have included some but not all of the features I want to include. There are leads I have yet to finish chasing, so the number of columns will likely increase over time.\n",
    "\n",
    "#### 2. Is EDA done?\n",
    "\n",
    "I am continuing to look at individual columns and how they correlate with the target. I look at both the Pearson and the Spearman correlation coefficients to help illuminate what potential usefulness a given feature may have for the final model. This is a constant work in progress.\n",
    "\n",
    "#### 3. Any modeling? How are predictions performing?\n",
    "\n",
    "AUC/ROC scores are rarely performing over 0.55. This is troublesome, but I have not given up hope that with the right columns and some grid searching, I can reach higher and higher scores. I have a soft goal of achieving a 0.70 AUC/ROC score.\n",
    "\n",
    "#### 4. Obstacles? (Processing, acquisition, cleaning, model issues)\n",
    "\n",
    "Processing power has been the primary limiting factor in my endeavors. I run lambda functions on a subset of the dataframe to get something that can be immediately plugged into a model. However, to look at correlation coefficients, I need a bigger subset or the entire dataframe to ensure there is an appropriate proportion of 1's and 0's in the target. Each column I attempt to add takes 5+ minutes to take shape, which makes my workflow disjointed and cumbersome.\n",
    "\n",
    "#### 5. Has topic changed? Enough progress to move forward?\n",
    "\n",
    "The topic has not yet changed, but if scores don't improve I have several ideas on how to pivot. I will cross that bridge if I get to it.\n",
    "\n",
    "#### 6. Timeline for next week and a half? Have to's versus would like to's.\n",
    "\n",
    "Utitlize AWS to overcome my processing power limitations. I can add a column, see if it helps, act upon that information, and then try another. I have several dozen potential columns that I truly believe will be helpful. I can later look into other signal transformations that may or may not have any usefulness at all. The stretch goal is to test every signal transformation method available in the library I use (there are around 46, I have looked through 12 so far). \n",
    "\n",
    "#### 7. Topics for 1:1\n",
    "\n",
    "How to read and tweak ROC/AUC scores; how to select features when addition of a seemingly highly correlated column seems to majorly worsen the results. I kept only three features and got the best results so far in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from glob import glob\n",
    "from scipy.io import wavfile\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import Audio\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assembling dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moreau = pd.read_csv('assets/moreau/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fado = pd.read_csv('assets/fado/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jacques = sorted(glob('assets/au-suivant/*.wav'))\n",
    "jacques = pd.DataFrame(jacques, columns=['filepath'])\n",
    "jacques = jacques.join(pd.read_csv('assets/au-suivant/results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "marling = sorted(glob('assets/marling/*.wav'))\n",
    "marling = pd.DataFrame(marling, columns=['filepath'])\n",
    "marling = marling.join(pd.read_csv('assets/marling/results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = moreau.append(fado)\n",
    "df = df.append(jacques)\n",
    "df = df.append(marling)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns that have shown to be helpful in classifying audio segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greater than .10 correlation coefficient for both Pearson and Spearman\n",
    "df['length'] = df['filepath'].apply(lambda x: len(librosa.core.load(x)[0]))\n",
    "df['stft_min_skewness'] = df['filepath'].apply(lambda x: np.min(sps.describe(librosa.core.stft(librosa.core.load(x)[0])).skewness))\n",
    "df['ifgram_min_corrcoef_variance'] = df['filepath'].apply(lambda x: np.min(np.corrcoef(sps.describe(librosa.core.ifgram(librosa.core.load(x)[0])).variance)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniyal/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:382: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  array = np.array(array, dtype=dtype, order=order, copy=copy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.919482686692\n",
      "0.586615287826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniyal/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:382: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  array = np.array(array, dtype=dtype, order=order, copy=copy)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.34537499,  0.32649686,  0.32812815])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.dropna(inplace=True)\n",
    "x = df.drop(['filepath', 'classification'], axis=1)\n",
    "y = df['classification']\n",
    "x_test, x_train, y_test, y_train = train_test_split(x, y)\n",
    "rfc = RandomForestClassifier(n_estimators=100, criterion='entropy')\n",
    "rfc.fit(x_train, y_train)\n",
    "pred_probs = rfc.predict_proba(x_test)\n",
    "predictions = []\n",
    "for x in range(len(pred_probs)):\n",
    "    predictions.append(pred_probs[x][1])\n",
    "print(rfc.score(x_train, y_train))\n",
    "print(rfc.score(x_test, y_test))\n",
    "print(roc_auc_score(y_test, predictions, average='samples'))\n",
    "rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
